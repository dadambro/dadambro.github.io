Machine learning
================
Damon D’Ambrosio
2023-07-19

## The most interesting machine learning method for me

I feel that random forests are the most interesting method of machine
learning. Briefly, random forests require creating multiple, independent
decision trees using a randomly selected subset of possible predictors.
The numerous trees (i.e., “forest”) are then “polled” to make a decision
regarding a prediction, with the idea of “majority rules.”

Relative to some other methods, like boosted trees, I like how the
random forest is a little more resistant to overfitting to “noise” in a
dataset. I also like that there are only a couple of tuning parameters
one can work with. However, at least on my lowly personal computer, the
random forest models can take a while to run :).

Below is a random forest model running on some data related to what I am
working on for Project 3. The model attempts to predict a Pokemon’s
“type” based on its base stats:

## Random forest

First, I will create a dataset. The code for the function used here, and
its helpers, can be found on my [Project
1](https://dadambro.github.io/project-1/) page.

``` r
#Create a dataset with all 1010 Pokemon and their base stats using the functions I created in Project 1
myData <- pokemon.batch.report(1:1010)

#Now add a binary variable denoting whether or not a Pokemon has a primary or secondary type of "bug"
myData <- myData %>% mutate(isBug = if_else(type1 == "bug" | type2 == "bug", 1, 0, 0))

#Remove unnecessary columns
myData <- myData %>% select(hp:isBug)

#Set isBug as a factor
myData$isBug <- as.factor(myData$isBug)
myData$isBug <- relevel(myData$isBug, ref = "1")
```

Now to set up the test and training datasets:

``` r
set.seed(80)

trainIndex <- createDataPartition(myData$isBug, p = 0.8, list = FALSE)

myTrain <- myData[trainIndex,]
myTest <- myData[-trainIndex,]
```

Now to train the random forest model. I will constrain the tuning grid
to use `mtry` values between 1 and 5, as there are 6 possible
predictors, and using all 6 is technically “bagging”:

``` r
randomForestFit <- train(isBug ~ ., data = myTrain, 
                  method = "rf",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "repeatedcv", number = 5, repeats = 3),
                  tuneGrid = data.frame(mtry = 1:5))
```

Check the best `mtry` value:

``` r
randomForestFit$bestTune
```

    ##   mtry
    ## 3    3

Now to run on the test data:

``` r
randomForestPredict <- predict(randomForestFit, newdata = myTest)

x <- confusionMatrix(randomForestPredict, myTest$isBug)
print(x)
```

    ## Confusion Matrix and Statistics
    ## 
    ##           Reference
    ## Prediction   1   0
    ##          1   0   3
    ##          0  18 180
    ##                                           
    ##                Accuracy : 0.8955          
    ##                  95% CI : (0.8447, 0.9342)
    ##     No Information Rate : 0.9104          
    ##     P-Value [Acc > NIR] : 0.80893         
    ##                                           
    ##                   Kappa : -0.0263         
    ##                                           
    ##  Mcnemar's Test P-Value : 0.00225         
    ##                                           
    ##             Sensitivity : 0.00000         
    ##             Specificity : 0.98361         
    ##          Pos Pred Value : 0.00000         
    ##          Neg Pred Value : 0.90909         
    ##              Prevalence : 0.08955         
    ##          Detection Rate : 0.00000         
    ##    Detection Prevalence : 0.01493         
    ##       Balanced Accuracy : 0.49180         
    ##                                           
    ##        'Positive' Class : 1               
    ## 

The random forest model had an accuracy of **0.8955224** on the test
data set. The highest accuracy it obtained on the training data set was
**0.9163587**.

Superficially, it seems the model is pretty accurate. However, I think
this might be an artifact of there only being 92 out of 1010 Pokemon
with either their primary or secondary typing being “bug.” Assuming I am
interpreting everything correctly, similar accuracy would be obtained by
simply guessing `0` for each Pokemon, as you would correctly “predict”
918 Pokemon by doing this.

Perhaps some of the other types will be more interesting. I will see as
Project 3 continues to unfold.
